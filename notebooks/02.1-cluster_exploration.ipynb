{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e32840",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6c2dd85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import statistics\n",
    "import os\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1361ce",
   "metadata": {},
   "source": [
    "### Read the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d2c67bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.DataFrame()\n",
    "for file in os.listdir():\n",
    "    if file.split('.')[-1]=='xlsx' and file.split('_')[0]=='train':\n",
    "        temp=pd.read_excel(file)\n",
    "        train_df=pd.concat([temp,train_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bbbc275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df=pd.DataFrame()\n",
    "for file in os.listdir():\n",
    "    if file.split('.')[-1]=='xlsx' and file.split('_')[0]=='val':\n",
    "        temp=pd.read_excel(file)\n",
    "        val_df=pd.concat([temp,val_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5a27c",
   "metadata": {},
   "source": [
    "### Take data subset with 50 observations from each category and apply lemmatization, stop word removal with spacy. Removing \"Bad Customer Service\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2a57a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df50=pd.DataFrame()\n",
    "for lab in train_df['label'].unique():\n",
    "    if lab not in ['bad customer service','customer feedback']:\n",
    "        df=train_df[train_df['label']==lab].iloc[:50]\n",
    "        df50=pd.concat([df,df50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e298fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list=df50['text'].tolist()\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "filtered_sent_list = []\n",
    "\n",
    "for idx,sentence in enumerate(sent_list):\n",
    "    # Process the sentence with spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    filtered_tokens=[token for token in filtered_tokens if token in vocab_words]\n",
    "    \n",
    "    # Join the filtered tokens into a new sentence\n",
    "    filtered_sentence = \" \".join(filtered_tokens)\n",
    "    \n",
    "    # Append the filtered sentence to the list\n",
    "    filtered_sent_list.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a0375",
   "metadata": {},
   "source": [
    "### Apply tfidf method to get the top-10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the sentences\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(filtered_sent_list)\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(sentence, n):\n",
    "    # Transform the given sentence using the trained TF-IDF model\n",
    "    tfidf_values = tfidf_vectorizer.transform([sentence])\n",
    "\n",
    "    # Convert the TF-IDF values to a dense array\n",
    "    tfidf_dense = tfidf_values.todense()\n",
    "\n",
    "    # Get the indices of the top N words\n",
    "    top_indices = np.argsort(tfidf_dense)[0, -n:][::-1]\n",
    "    top_indices=[i for i in top_indices.tolist()[0]]\n",
    "\n",
    "    # Get the corresponding words from the feature names\n",
    "    top_words = [feature_names[i] for i in top_indices]\n",
    "\n",
    "    return top_words\n",
    "\n",
    "\n",
    "## Generate top 10 words list\n",
    "\n",
    "top10_words_list=[]\n",
    "for sent in filtered_sent_list:\n",
    "    top10_words_list.append(get_top_n_words(sent,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e8647",
   "metadata": {},
   "source": [
    "### Using word2vec embeddings for similarity and create a similarity summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a69d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list=df50['label'].tolist()\n",
    "\n",
    "final_sim_list=[]\n",
    "for idx,words in enumerate(top10_words_list):\n",
    "    print(idx)\n",
    "    sim_dict={}\n",
    "    for lab in list(set(label_list)):\n",
    "        sim_list=[]\n",
    "        for word in words:\n",
    "            if all(word in word2vec_model.key_to_index for word in [word] + lab.split()):\n",
    "                word_vec = word2vec_model[word]\n",
    "                lab_vec=word2vec_model[lab.split()].mean(axis=0)\n",
    "                \n",
    "                similarity = cosine_similarity([word_vec], [lab_vec])[0][0]\n",
    "                \n",
    "                sim_list.append(similarity)\n",
    "        if len(sim_list)>0:\n",
    "            sim_dict[lab]=(max(sim_list),statistics.mean(sim_list))\n",
    "        else:\n",
    "            sim_dict[lab]=(0,0)\n",
    "        \n",
    "    final_sim_list.append(sim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc630c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list=[]\n",
    "for idx,i in enumerate(final_sim_list):\n",
    "    temp={}\n",
    "    temp['predicted']=[(key, final_sim_list[idx][key]) for key in sorted(final_sim_list[idx], key=lambda k: final_sim_list[idx][k][0], reverse=True)[:5]]\n",
    "    temp['actual']=(label_list[idx],final_sim_list[idx][label_list[idx]])\n",
    "    final_list.append(temp) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda02b8a",
   "metadata": {},
   "source": [
    "### Final Cluster Categories after creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf03e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1=['account cancellation','account security','login issues',\n",
    "           'forgot my password','software update']   \n",
    "\n",
    "\n",
    "cluster_2=['best buy credit card','payment failed','billing or charge disputes','cancel order',\n",
    "'unauthorized charge or payment','refund request','fraud concerns','return request',\n",
    "'cancellation of a plan subscription or membership','account cancellation','change or update order',\n",
    "'schedule order pickup','change shipping time','delivery tracking','refund status',\n",
    "'change payment method','payment method','change shipping address',\n",
    "'delivery or parts of delivery items missing','renewal of a plan subscription or membership',\n",
    "'reschedule delivery','reschedule order pickup','rewards or discounts','schedule delivery',\n",
    "'trade in inquiry','delivery delays']\n",
    "\n",
    "\n",
    "cluster_3=['check warranty coverage','damaged product','warranty claim','reschedule repair',\n",
    "'device damaged','incomplete installation','lost or forgot items','reschedule installation',\n",
    "'schedule repair','screen issues','software error','software installation','schedule installation',\n",
    "'troubleshooting','performance issues','defective product']\n",
    "\n",
    "\n",
    "cluster_4=['employment or career inquiries','website or app complaints','incomplete installation',\n",
    "'miscellaneous inquiries','network or connectivity issues','customer feedback','bad customer service']\n",
    "\n",
    "\n",
    "cluster_5=['price match','product availability and stock','product compatibility',\n",
    "'product details inquiry','transfer call to the right department or store']\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dva_lstm",
   "language": "python",
   "name": "dva_lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
