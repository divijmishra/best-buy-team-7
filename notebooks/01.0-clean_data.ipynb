{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to clean and save raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import time\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directories for data\n",
    "!mkdir -p data/clean\n",
    "!mkdir -p data/interim/tfidfs\n",
    "!mkdir -p data/interim/vectorizers\n",
    "!mkdir -p data/interim/vocabs\n",
    "!mkdir -p data/raw\n",
    "!mkdir -p data/results/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if required\n",
    "# !python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PREPROCESSING FUNCTION\n",
    "\n",
    "# apply preprocessing: stop-word removal, lemmatization\n",
    "# will take a long time! experiment with small batches first.\n",
    "\n",
    "def nlp_preprocess(df):\n",
    "    docs = df[\"text\"].tolist()\n",
    "\n",
    "    num_docs = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    preprocessed_texts = []\n",
    "\n",
    "    for doc in nlp.pipe(docs):\n",
    "        # use this instead of \"for doc in docs\" for memory efficiency\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if not (token.is_stop or token.is_punct or token.is_space):\n",
    "                tokens.append(token.lemma_)\n",
    "\n",
    "        text = \" \".join(tokens)\n",
    "        preprocessed_texts.append(text)\n",
    "\n",
    "        # timing for Google Colab tracking\n",
    "        num_docs += 1\n",
    "        if num_docs % 10000 == 0:\n",
    "            curr_time = time.time()\n",
    "            print(\n",
    "                f\"Cleaned: {num_docs}/{len(docs)} docs. Time elapsed: {((curr_time - start_time) / 60):.1f} min.\"\n",
    "            )\n",
    "\n",
    "    df[\"text_preproced\"] = preprocessed_texts\n",
    "    df = df[[\"label\", \"text_preproced\", \"text\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and save train df\n",
    "load_path = \"../data/raw/training_dataset.xlsx\"\n",
    "df = pd.read_excel(load_path)\n",
    "\n",
    "print(\"Loaded dataset.\")\n",
    "\n",
    "df_clean = nlp_preprocess(df)\n",
    "print(\"Cleaned dataset.\")\n",
    "\n",
    "save_path = \"../data/clean/training_data_cleaned.xlsx\"\n",
    "df_clean.to_excel(save_path)\n",
    "print(\"Saved dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and save validation df\n",
    "load_path = \"../data/raw/validation_dataset.xlsx\"\n",
    "df = pd.read_excel(load_path)\n",
    "\n",
    "print(\"Loaded dataset.\")\n",
    "\n",
    "df_clean = nlp_preprocess(df)\n",
    "print(\"Cleaned dataset.\")\n",
    "\n",
    "save_path = \"../data/clean/validation_data_cleaned.xlsx\"\n",
    "df_clean.to_excel(save_path)\n",
    "print(\"Saved dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and save test df\n",
    "load_path = \"../data/raw/test_dataset.xlsx\"\n",
    "df = pd.read_excel(load_path)\n",
    "\n",
    "print(\"Loaded dataset.\")\n",
    "\n",
    "df_clean = nlp_preprocess(df)\n",
    "print(\"Cleaned dataset.\")\n",
    "\n",
    "save_path = \"../data/clean/test_data_cleaned.xlsx\"\n",
    "df_clean.to_excel(save_path)\n",
    "print(\"Saved dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save just the labels - used in test inference script\n",
    "data_types =  [\"test\", \"validation\", \"training\"]\n",
    "\n",
    "for data_type in data_types:\n",
    "    load_path = f\"../data/raw/{data_type}_dataset.xlsx\"\n",
    "    df = pd.read_excel(load_path)\n",
    "    df.drop(columns=[\"text\"], inplace=True)\n",
    "    df.to_excel(f\"../data/clean/{data_type}_labels.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
